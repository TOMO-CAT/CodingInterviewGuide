# 低延迟

## 总体思路

* 减少网络 IO
* 减少硬盘 IO
* 减少系统调用
* 优化代码：
  * 减少上下文切换；
  * 减少系统调用；
  * 避免锁竞争；
  * CPU cache 利用率；

## 优化代码

### 1. 限制动态内存分配

**相关的知识背景：**glibc 默认的 malloc 背后有复杂的算法，当堆空间不足时会调用 sbrk()，当分配内存很大时会调用 mmap()，这些都是系统调用，似乎会比较慢，而且新分配的内存被 first touch 时也要过很久才能准备好。

**可取的做法**：

* 尽量使用 vector 或者 array（初始化时分配足够的空间，之后每次使用都从里面取出来用）
* 尽量使用内存池
* 如果需要二叉树或者哈希表，尽量使用侵入式容器（boost::intrusive）

### 2. 使用自旋，避免上下文切换

**相关的知识背景**：上下文切换是非常耗时的，其中固定的消耗包括（cpu 流水线被冲掉、各种寄存器需要被保存和恢复、内核中的调度算法要被执行），此外，缓存很有可能出现大量 miss，这属于不固定的时间消耗。

**可取的做法：**使用带有内核 bypass 功能的网卡。每个进程或者线程都独占一个 cpu 核，并且不停地轮询，用以保证快速响应。尽量避免任何可能导致阻塞的事件（如 mutex），某些注定很慢的活动（比如把 log 写到磁盘上）应该被独立出来放到别的 cpu 上，不能影响主线程。

### 3. 使用共享内存作为唯一的 IPC 机制

**相关的知识背景：**共享内存只有在初始化的时候有一些系统调用，之后就可以像访问正常内存一样使用了。**其他 IPC 机制（管道、消息队列、套接字）则是每次传输数据时都有系统调用**，**并且每次传输的数据都经历多次拷贝**。因此共享内存是最快的 IPC 机制。

**可取的做法：**使用共享内存作为唯一的 IPC 机制。当然，可能需要手动实现一些东西来保证共享的数据在多进程下是安全，我们是自己实现了无锁内存池、无锁队列和顺序锁。

### 4. 传送消息时使用无锁队列

**相关的知识背景**：我只关注基于数组的无锁队列，其中：spsc 队列是 wait-free 的，不论是入队出队都可以在确定的步数之内完成，而且实现时只需要基本的原子操作【为什么这很重要见注释 7】；mpmc 队列的实现方式则多种多样，但都会稍微慢一点，因为它们需要用一些比较重的原子操作（CAS 或者 FAA），而且有时它们需要等待一段不确定的时间直到另一个线程完成相应操作；另外，还有一种 multi-observer 的『广播队列』，多个读者可以收到同一条消息广播，这种队列也有 sp 和 mp 类型的，可以检查或者不检查 overwrite；最后，还有一种队列允许存储不定长的消息。

**可取的做法**：总的来说，应该避免使用 mp 类型的队列，举例：如果要用 mpsc 队列，可以使用多个 spsc 来达成目的，并不需要 mp 队列；同理，如果是消息广播，也可以使用多个 sp 队列来取代一个 mp 队列；如果广播时 observer 只想订阅一部分消息，那么可以用多个 spsc+ 有计数功能的内存池【具体做法见注释 2】；如果要求多个观察者看到多个生产者的消息，并且顺序一致，那只能用 mp 队列了。总结一下，mp 类型的队列应该尽量避免，因为当多个生产者同时抢占队列的时候，延时会线性增长。

### 5. 考虑缓存对速度的影响

**相关的背景知识：**现在的机器内存是十分充足的，但是缓存还是很小，因此所有节省内存的技巧都还有用武之地。

**可取的做法：**

* 尽量让可能被同时使用的数据挨在一起；减少指针链接（比如用 array 取代 vector，因为链接指向的地方可能不在缓存里）
* 尽量节省内存（比如用 unique_ptr<Data[]>取代 vector<Data>，比如成员变量按照从大到小排序，比如能用 int8 的地方就不用 int16）
* 指定 cpu affinity 时考虑 LLC 缓存（同核的两个超线程是共享 L1，同 cpu 的两个核是共享 L3，不同 NUMA 核是通过 QPI 总线）
* 会被多个核同时读写的数据按照缓存行对齐（避免 false sharing）

## Reference

[1] <https://www.zhihu.com/question/23185359/answer/936467060>

[2] <https://www.zhihu.com/question/23185359/answer/936467060>
